<DOCTYPE html>
    <html lang="en">


    <head>
        <meta charset="UTF-8" />
        <title>Shihao Zou</title>
        <meta name="description" content="Shihao Zou's personal website." />
        <meta name="keywords" content="computer vision, machine learning" />
        <link rel="stylesheet" href="css/style.css">
    </head>

    <style>
        body {
            max-width: 100ch;
            min-height: 80vh;
        }

        h1 {
            font-size: 2.0rem;
        }

        h2 {
            font-size: 1.2rem;
        }

        h3 {
            font-size: 1.0rem;
        }

        h4 {
            font-size: 1.0rem;
        }
    </style>

    <body>
        <h1 style="text-align:center;">Shihao Zou 邹诗浩
            <label for="sn-1" class="sidenote-toggle sidenote-number"></label>
            <input type="checkbox" id="sn-1" class="sidenote-toggle" />
            <span class="sidenote right">
                A photo of myself
                <img src="assets/photo2023.jpg" loading="lazy" alt="Shihao Zou's photo" width="260" />
            </span><br>
        </h1>
        <p class="author">
            <a href="https://github.com/JimmyZou">[Github]</a>
            <a href="https://scholar.google.com/citations?user=FFm6HDkAAAAJ&hl=en">[Google Scholar]</a>
            <!-- <a href="https://www.linkedin.com/in/shihao-zou-724177202">[Linkedin]</a> -->
            <a href="./assets/CV.pdf" download="CV_Shihao_Zou.pdf">[CV]</a>
            <br>Email: sh.zou@siat.ac.cn<br>
            Last Update: June 23, 2025
        </p>

        <div class="abstract">
            <h2>Bios</h2>

            <p style="text-align:left;">
                I am a Research Assistant Professor at the <a href="http://mmlab.siat.ac.cn/">Multimedia Lab</a>
                <label for="sn-2" class="sidenote-toggle sidenote-number"></label>
                <input type="checkbox" id="sn-2" class="sidenote-toggle" />
                <span class="sidenote right">
                    Multimedia Lab at SIAT was founded by Professor <a
                        href="https://www.ie.cuhk.edu.hk/people/xotang.shtml">Xiaoou Tang</a> and <a
                        href="http://mmlab.siat.ac.cn/yuqiao/">Yu Qiao</a>, and is currently led by Professor <a
                        href="http://xpixel.group/2010/01/20/chaodong.html">Chao Dong</a>.
                </span>
                of the Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences (<b>SIAT, CAS</b>).
                If you are interested in collaborating on research or have any
                other inquiries, please don't hesitate to contact me. Thank you for visiting!
            </p>
            <br>

            <p style="text-align:left;">
                I graduated from University of Alberta (<b>UoA</b>) in Canada with Ph.D. degree in Computer Engineering
                advised by Professor <a href="https://www.ece.ualberta.ca/~lcheng5/">Li Cheng</a>. During
                my Ph.D., I was fortunate to intern at Meta Reality Labs twice, advised by Dr. <a
                    href="https://web.cs.ucla.edu/~yuanluxu/index.html">Yuanlu Xu</a> in 2023 and Dr. <a
                    href="https://minhpvo.github.io/">Minh Vo</a> in 2022, and also at Huawei 2012 Labs twice.
                Before Ph.D., I graduated from University College London (<b>UCL</b>) with M.Res. degree advised by
                Professor <a href="http://www0.cs.ucl.ac.uk/staff/jun.wang/">Jun Wang</a>
                and Beijing Institute of Technology (<b>BIT</b>) with B.Sc. degree advised by Professor <a
                    href="https://isc.bit.edu.cn/schools/iae/knowinprofessors10/b113101.htm">Huiqi Li</a>.
            </p>
            <br>

            <p style="text-align:left;">
                My research interests lie in computer vision, brain-inspired computating, and medical data analysis.
            <ul style="text-align:left;">
                <li>
                    <b>Brain-inspired Computing</b>: spiking neural networks, spike-driven Transformers,
                    spike-driven generation models, and neuromorphic computing.
                </li>
                <li>
                    <b>Digital Humans</b>: motion capture, motion synthesis/generation
                    and human shape reconstruction.
                </li>
                <li>
                    <b>Computer Vision and Novel Sensors</b>: event, polarization, and light field cameras
                    with their applications in vision tasks.
                </li>
            </ul>
            </p>
        </div>

        <h2 id="blogs">Blogs Posted</h2>
        <ul>
            <li>
                <a href="src/blog_generative_models.html">Blog: Generative Models Related</a><br>
            </li>
            <li>
                <a href="src/blog_graduate_guidance.html">Blog: Graduate Student Onboarding 研究生入门指南</a><br>
            </li>
            <li>
                <a href="src/blog_human_shape_recon.html">Blog: Human Shape Reconstruction</a><br>
            </li>
            <li>
                <a href="src/blog_event_based_vision.html">Blog: Event-Based Vision and Some Other Research
                    Topics</a><br>
            </li>
        </ul>

        <main>
            <article>
                <h2 id="publications">Publications</h2>
                <span>✉</span> indicates corresponding author. * indicates equal contribution.
                <table cellspacing="15" style="border:hidden;">
                    <tbody>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/icml2025.jpg" border="0">
                                &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with
                                        Hamming Attention and O(T) Complexity</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>,
                                    Qingfeng Li,
                                    <a href="https://scholar.google.com/citations?user=G4uCKHcAAAAJ&hl=en">Wei Ji</a>,
                                    <a href="https://vision-and-learning-lab-ualberta.github.io/author/jingjing-li/">Jingjing
                                        Li</a>,
                                    <a href="https://people.ucas.ac.cn/~YangYongkui">Yongkui Yang</a>,
                                    <a href="https://casialiguoqi.github.io/">Guoqi Li</a>,
                                    <a href="http://xpixel.group/">Chao Dong</a>.<br>
                                    In ICML 2025. <br>
                                    [<a href="https://arxiv.org/abs/2505.10352">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    [<a href="https://github.com/JimmyZou/SpikeVideoFormer">Code</a>]
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/miccai2025.jpg"
                                    border="0">
                                &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Cerebrovascular Diseases Screening from Color Fundus Photography via Cross-View
                                        Fusion and Graph-Based Discrimination</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    Congyu Tian*,
                                    <u><a href="https://jimmyzou.github.io/">Shihao Zou</a>*</u>,
                                    Xiangyun Liao,
                                    <a href="https://cchen-cc.github.io/">Cheng Chen</a>,
                                    Chubin Ou,
                                    Jianping Lv,
                                    <a href="https://people.ucas.edu.cn/~ShanshanWang?language=en">Shanshan Wang</a>,
                                    <a href="https://scholar.google.com/citations?user=E4efwTgAAAAJ&hl=zh-TW">Weixin
                                        Si</a>.<br>
                                    In MICCAI 2025. <br>
                                    <!-- [<a href="">Paper</a>] -->
                                    <!-- [<a href="">Webpage</a>] -->
                                    [<a href="https://github.com/glodxy/CVGB_net">Code</a>]
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/gom2025.png" border="0">
                                &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Semantics-Aware Human Motion Generation from Audio Instructions</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    Zian Wang, <u><a href="https://jimmyzou.github.io/">Shihao
                                            Zou</a><span>✉</span></u>,
                                    Shiyao Yu,
                                    <a href="https://mingyuan-zhang.github.io/">Mingyuan Zhang</a>,
                                    <a href="http://xpixel.group/">Chao Dong</a>.<br>
                                    In CVM 2025 Oral (Recommended to Graphical Models Elsevier, JCR Q2). <br>
                                    [<a href="https://dl.acm.org/doi/10.1145/3712011">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    [<a href="">Code</a>]
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/tomm2025.jpg" border="0">
                                &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Generating High-Fidelity Clothed Human Dynamics with
                                        Temporal Diffusion</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>,
                                    <a href="https://scholar.google.com/citations?user=fvr-J3sAAAAJ&hl=en">Yuanlu
                                        Xu</a>,
                                    <a href="https://nsarafianos.github.io/">Nikolaos Sarafianos</a>,
                                    <a href="https://fbogo.github.io/">Federica Bogo</a>,
                                    <a href="https://sites.google.com/site/tony2ng/">Tony Tung</a>,
                                    <a href="https://scholar.google.com/citations?user=E4efwTgAAAAJ&hl=zh-TW">Weixin
                                        Si</a>,
                                    <a href="https://vision-and-learning-lab-ualberta.github.io/">Li Cheng</a>.<br>
                                    In ACM TOMM 2025 (JCR Q1). <br>
                                    [<a href="https://dl.acm.org/doi/10.1145/3712011">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    [<a href="">Code</a>]
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>


                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/eventhpe_snn.jpg"
                                    border="0"> &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Highly Efficient 3D Human Pose Tracking from Events with Spiking Spatiotemporal
                                        Transformer</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>,
                                    <a href="https://yxmu.foo/">Yuxuan Mu</a>,
                                    <a href="https://scholar.google.com/citations?user=G4uCKHcAAAAJ&hl=en">Wei Ji</a>,
                                    Zi-An Wang,
                                    <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>,
                                    <a href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>,
                                    <a href="https://scholar.google.com/citations?user=E4efwTgAAAAJ&hl=zh-TW">Weixin
                                        Si</a>,
                                    <a href="https://vision-and-learning-lab-ualberta.github.io/">Li Cheng</a>.<br>
                                    In IEEE TCSVT 2025 (JCR Q1). <br>
                                    [<a href="https://arxiv.org/abs/2303.09681">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    [<a href="https://github.com/JimmyZou/HumanPoseTracking_SNN">Code</a>]
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/snipper.jpg" border="0">
                                &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Snipper: A Spatiotemporal Transformer
                                        for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a
                                        Video Snippet</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>, <a
                                        href="https://web.cs.ucla.edu/~yuanluxu/">Yuanlu Xu</a>, Chao Li,
                                    Lingni Ma, <a href="https://vision-and-learning-lab-ualberta.github.io/">Li
                                        Cheng</a>, Minh Vo.<br>
                                    In IEEE TCSVT 2023 (JCR Q1). <br>
                                    [<a href="https://arxiv.org/abs/2207.04320.pdf">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    [<a href="">Code</a>]
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/polar_hpe.jpg" border="0">
                                &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Human Pose and Shape Estimation from Single Polarization Images</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>, <a
                                        href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a
                                        href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>,
                                    Yiming Qian, <a href="https://ericguo5513.github.io/">Chuan Guo</a>, Li
                                    Cheng.<br>
                                    In IEEE TMM 2022 (JCR Q1). <br>
                                    [<a href="https://arxiv.org/abs/2108.06834">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    [<a href="">Code</a>]
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/eventhpe.jpg" border="0">
                                &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>EventHPE: Event-based 3D Human Pose Estimation</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>, <a
                                        href="https://ericguo5513.github.io/">Chuan Guo</a>, <a
                                        href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>,
                                    <a href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>, Pengyu Wang,
                                    Xiaoqin
                                    Hu, Shoushun Chen, <a href="https://socs.uoguelph.ca/~minglun/">Minglun Gong</a>, <a
                                        href="https://vision-and-learning-lab-ualberta.github.io/">Li Cheng</a>.<br>
                                    In ICCV 2021. <br>
                                    [<a href="https://arxiv.org/abs/2108.06819">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    [<a href="">Code</a>]
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/polar_hpe_eccv.jpg"
                                    border="0"> &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>3D Human Shape Reconstruction from a Polarization Image</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>, Yiming Qian, <a
                                        href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>,
                                    <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, Chi Xu,
                                    Minglun
                                    Gong, <a href="https://vision-and-learning-lab-ualberta.github.io/">Li
                                        Cheng</a>.<br>
                                    In ECCV 2020. <br>
                                    [<a href="https://arxiv.org/abs/2007.09268">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    [<a href="">Code</a>]
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/multiagent_rank.jpg"
                                    border="0"> &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Multi-agent Reinforced Learning to Rank</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>, Zhonghua Li, Mohammad
                                    Akbari, <a href="http://www0.cs.ucl.ac.uk/staff/jun.wang/">Jun Wang</a>, Peng
                                    Zhang.<br>
                                    In CIKM 2020. <br>
                                    [<a href="https://arxiv.org/abs/1909.06859">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    <!-- [<a href="">Code</a>] -->
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/ictir.jpg" border="0">
                                &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>On the Equilibrium of Query Reformulation and Document Retrieval</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>, Guanyu Tao, <a
                                        href="http://www0.cs.ucl.ac.uk/staff/jun.wang/">Jun Wang</a>,
                                    <a href="https://wnzhang.net/">Weinan Zhang</a>, Dell Zhang.<br>
                                    In ICTIR 2018. <br>
                                    [<a href="https://arxiv.org/abs/1807.02299">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    <!-- [<a href="">Code</a>] -->
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center"
                                    src="./assets/fig_publication/trimodel_motion_retrieval.jpg" border="0"> &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Tri-Modal Motion Retrieval by Learning a Joint Embedding Space</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <a href="https://github.com/yinkangning0124">Kangning Yin</a>, <u><a
                                            href="https://jimmyzou.github.io/">Shihao Zou</a></u>, Yuxuan Ge, <a
                                        href="https://sca.shanghaitech.edu.cn/sca_en/2020/0903/c7933a173623/page.htm">Zheng
                                        Tian</a>.<br>
                                    In CVPR 2024. <br>
                                    [<a href="https://arxiv.org/abs/2403.00691">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    [<a href="">Code</a>]
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/racon23.jpg" border="0">
                                &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>RACon: Retrieval-Augmented Simulated Character Locomotion Control</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <a href="https://yxmu.foo/">Yuxuan Mu</a>, <u><a
                                            href="https://jimmyzou.github.io/">Shihao Zou</a></u>, <a
                                        href="https://github.com/yinkangning0124">Kangning Yin</a>, <a
                                        href="https://sca.shanghaitech.edu.cn/sca_en/2020/0903/c7933a173623/page.htm">Zheng
                                        Tian</a>, <a href="https://vision-and-learning-lab-ualberta.github.io/">Li
                                        Cheng</a>, <a href="https://wnzhang.net/">Weinan Zhang</a>, <a
                                        href="http://www0.cs.ucl.ac.uk/staff/jun.wang/">Jun Wang</a>.<br>
                                    In ICME 2024. <br>
                                    [<a href="https://arxiv.org/abs/2403.00691">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    [<a href="">Code</a>]
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/text2motion.jpg"
                                    border="0"> &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Diverse and Natural 3D Human Motions from Text</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <a href="https://ericguo5513.github.io/">Chuan Guo</a>, <u><a
                                            href="https://jimmyzou.github.io/">Shihao Zou</a></u>, <a
                                        href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>,
                                    <a href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>, Wei Ji, <a
                                        href="https://vision-and-learning-lab-ualberta.github.io/">Li
                                        Cheng</a>.<br>
                                    In CVPR 2022. <br>
                                    [<a
                                        href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    <!-- [<a href="">Code</a>] -->
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/action2video.jpg"
                                    border="0"> &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Action2video: Generating Videos of Human 3D Actions</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <a href="https://ericguo5513.github.io/">Chuan Guo</a>, <a
                                        href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a
                                        href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>, Xinshuang
                                    Liu, <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>, Minglun
                                    Gong, <a href="https://vision-and-learning-lab-ualberta.github.io/">Li
                                        Cheng</a>.<br>
                                    In IJCV 2022 (JCR Q1). <br>
                                    [<a href="https://arxiv.org/abs/2111.06925">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    <!-- [<a href="">Code</a>] -->
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/action2motion.jpg"
                                    border="0"> &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Action2Motion: Conditioned Generation of 3D Human Motions</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <a href="https://ericguo5513.github.io/">Chuan Guo</a>, <a
                                        href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a
                                        href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>, <u><a
                                            href="https://jimmyzou.github.io/">Shihao
                                            Zou</a></u>, Qingyao Sun, Annan
                                    Deng, <a href="https://socs.uoguelph.ca/~minglun/">Minglun Gong</a>, <a
                                        href="https://vision-and-learning-lab-ualberta.github.io/">Li
                                        Cheng</a>.<br>
                                    In ACM MM 2020. <br>
                                    [<a href="https://arxiv.org/abs/2007.15240">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    <!-- [<a href="">Code</a>] -->
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/opponent_modeling.jpg"
                                    border="0"> &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Learning to Communicate Implicitly By Actions</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <a href="https://sca.shanghaitech.edu.cn/sca_en/2020/0903/c7933a173623/page.htm">Zheng
                                        Tian</a>, <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>, Ian
                                    Davies, Tim Warr, Lisheng Wu, Haitham
                                    Bou Ammar, <a href="http://www0.cs.ucl.ac.uk/staff/jun.wang/">Jun Wang</a>.<br>
                                    In AAAI 2020 (spotlight). <br>
                                    [<a href="https://arxiv.org/abs/1810.04444">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    <!-- [<a href="">Code</a>] -->
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="20%" align=center>
                                <img width="200" align="center" src="./assets/fig_publication/opponent_ijcai.jpg"
                                    border="0"> &nbsp;
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b>A Regularized Opponent Model with Maximum Entropy Objective</b>
                                </span>
                                <br>
                                <span style="font-size: 11pt;">
                                    <a href="https://sca.shanghaitech.edu.cn/sca_en/2020/0903/c7933a173623/page.htm">Zheng
                                        Tian</a>, <a href="https://yingwen.io/">Ying Wen</a>, Zhichen Gong, Faiz
                                    Punakkath, <u><a href="https://jimmyzou.github.io/">Shihao Zou</a></u>, <a
                                        href="http://www0.cs.ucl.ac.uk/staff/jun.wang/">Jun Wang</a>.<br>
                                    In IJCAI 2019. <br>
                                    [<a href="https://arxiv.org/abs/1905.08087">Paper</a>]
                                    <!-- [<a href="">Webpage</a>] -->
                                    <!-- [<a href="">Code</a>] -->
                                    <!-- [<a href="">Bibtex</a>] -->
                                </span><br>
                            </td>
                        </tr>

                    </tbody>
                </table>

                <h2 id="services">Services</h2>
                <ul>
                    <li>Reviewer @ ICML2025, ICCV 2025, CVPR2024, ECCV2024, AAAI2024, ICCV2023, CVPR2022, ECCV2022,
                        CVPR2021, ICCV2021, AAAI2021, IJCAI2021</li>
                    <li>Invited Reviewer @ IEEE TPAMI, IEEE TMM, IEEE TCSVT, PR, IEEE Sensors</li>
                </ul>

                <h2 id="experiences">Experiences</h2>
                <ul>
                    <li>
                        Jun 2023 ~ Sept 2023 - Research Internship @ Huawei 2012 Labs, Canada
                    </li>
                    <li>
                        Jul 2022 ~ Nov 2022 - Research Scientist Intern @ Meta Reality Labs, Switzerland
                    </li>
                    <li>
                        May 2021 ~ Dec 2021 - Research Scientist Intern @ Meta Reality Labs, Canada
                    </li>
                    <li>
                        Sep 2018 ~ Jan 2019 - Research Internship @ Huawei 2012 Labs, China
                    </li>
                </ul>

                <!-- <h2 id="education">Education</h2>
                <ul>
                    <li>
                        Jan 2019 ~ Dec 2023 <br>
                        Ph.D. in Computer Engineering <br>
                        University of Alberta, Canada
                    </li>
                    <li>
                        Sep 2017 ~ Sep 2018 <br>
                        M.Res. in Web Science and Big Data Analytics <br>
                        University College London, UK
                    </li>
                    <li>
                        Sep 2013 ~ Jun 2017 <br>
                        B.Sc. in Electronic Information Engineering<br>
                        Beijing Institute of Technology, China
                    </li>
                    <li>
                        Sep 2016 ~ Jan 2017 <br>
                        Visiting <br>
                        Hong Kong University of Science and Technology
                    </li>
                </ul> -->

            </article>
        </main>
    </body>

    <hr>

    <footer>
        <p>
            Made using <a href="https://latex.vercel.app">https://latex.vercel.app</a> and followed my genius friend
            <a href="https://lzhou1110.github.io/">Liyi Zhou</a> &#128077;&#128077;&#128077;.
        </p>
    </footer>

    </html>
</DOCTYPE>